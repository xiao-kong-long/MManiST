{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22adf6c2",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98864f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\application\\Anaconda3\\envs\\stagate_env1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:\\\\pyprog\\\\papers_code\\\\hyperbolic_project')\n",
    "import GraphZoo.graphzoo as gz\n",
    "import torch\n",
    "from GraphZoo.graphzoo.config import parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d6752",
   "metadata": {},
   "source": [
    "# Defining Parameters\n",
    "\n",
    "The config file in the source is used to define the parameters. Users can define it in the following way using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc63b449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining Parameters\n",
    "params = parser.parse_args(args=[])\n",
    "\n",
    "#Data Parameters\n",
    "params.dataset='cora'\n",
    "params.datapath='data/cora'\n",
    "\n",
    "#Model Parameters\n",
    "params.task='nc'\n",
    "params.model='HGCN'\n",
    "params.manifold='PoincareBall'\n",
    "params.dim=128\n",
    "\n",
    "#Training Parameters\n",
    "params.lr=0.01\n",
    "params.weight_decay=0.001\n",
    "params.dropout=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8b5ac",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Using the data parameters defined by the user, raw data is converted to desired graph data which is feature matrix, adjacency matrix and labels. After that the data is split into train, validation and test set depending on parameters given by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65df9dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/cora\\\\ind.cora.x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m gz\u001b[39m.\u001b[39;49mdataloader\u001b[39m.\u001b[39;49mDataLoader(params)\n",
      "File \u001b[1;32mD:\\pyprog\\papers_code\\hyperbolic_project\\GraphZoo\\graphzoo\\dataloader\\dataloader.py:34\u001b[0m, in \u001b[0;36mDataLoader\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mGraphZoo Dataloader\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 34\u001b[0m     data \u001b[39m=\u001b[39m load_data_nc(args\u001b[39m.\u001b[39;49mdataset, args\u001b[39m.\u001b[39;49muse_feats, args\u001b[39m.\u001b[39;49mdatapath, args\u001b[39m.\u001b[39;49msplit_seed)\n\u001b[0;32m     35\u001b[0m     args\u001b[39m.\u001b[39mn_classes \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmax() \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\pyprog\\papers_code\\hyperbolic_project\\GraphZoo\\graphzoo\\dataloader\\dataloader.py:81\u001b[0m, in \u001b[0;36mload_data_nc\u001b[1;34m(dataset, use_feats, data_path, split_seed)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data_nc\u001b[39m(dataset, use_feats, data_path, split_seed):\n\u001b[0;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m dataset \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mcora\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mciteseer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpubmed\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m---> 81\u001b[0m         adj, features, labels, idx_train, idx_val, idx_test \u001b[39m=\u001b[39mload_citation_data(\n\u001b[0;32m     82\u001b[0m             dataset, use_feats, data_path)\n\u001b[0;32m     83\u001b[0m     \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mppi\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     84\u001b[0m         adj, features, labels \u001b[39m=\u001b[39m load_ppi_data(data_path)\n",
      "File \u001b[1;32mD:\\pyprog\\papers_code\\hyperbolic_project\\GraphZoo\\graphzoo\\dataloader\\dataloader.py:229\u001b[0m, in \u001b[0;36mload_citation_data\u001b[1;34m(dataset_str, use_feats, data_path)\u001b[0m\n\u001b[0;32m    227\u001b[0m objects \u001b[39m=\u001b[39m []\n\u001b[0;32m    228\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(names)):\n\u001b[1;32m--> 229\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, \u001b[39m\"\u001b[39;49m\u001b[39mind.\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(dataset_str, names[i])), \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    230\u001b[0m         \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info \u001b[39m>\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m0\u001b[39m):\n\u001b[0;32m    231\u001b[0m             objects\u001b[39m.\u001b[39mappend(pkl\u001b[39m.\u001b[39mload(f, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/cora\\\\ind.cora.x'"
     ]
    }
   ],
   "source": [
    "data = gz.dataloader.DataLoader(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67844b",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1728e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model= gz.models.NCModel(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285dab6b",
   "metadata": {},
   "source": [
    "# Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9fffcaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Defining Optimizer\n",
    "optimizer = gz.optimizers.RiemannianAdam(params=model.parameters(), \n",
    "                                         lr=params.lr, weight_decay=params.weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b31cb6",
   "metadata": {},
   "source": [
    "# Training and Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a689c7ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using: cpu\n",
      "INFO:root:Using seed 1234.\n",
      "INFO:root:NCModel(\n",
      "  (encoder): HGCN(\n",
      "    (layers): Sequential(\n",
      "      (0): HyperbolicGraphConvolution(\n",
      "        (linear): HypLinear(in_features=1433, out_features=128, c=tensor([1.]))\n",
      "        (agg): HypAgg(c=tensor([1.]))\n",
      "        (hyp_act): HypAct(c_in=tensor([1.]), c_out=tensor([1.]))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): LinearDecoder(\n",
      "    in_features=128, out_features=7, bias=1, c=tensor([1.])\n",
      "    (cls): Linear(\n",
      "      (linear): Linear(in_features=128, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "INFO:root:Total number of parameters: 184455\n",
      "/opt/anaconda3/lib/python3.8/site-packages/graphzoo-0.0.1-py3.8.egg/graphzoo/manifolds/poincare.py:99: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/graphzoo-0.0.1-py3.8.egg/graphzoo/optimizers/radam.py:129: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0005 lr: 0.01 train_loss: 1.9406 train_acc: 0.1786 train_f1: 0.1786 time: 0.0769s\n",
      "INFO:root:Epoch: 0005 val_loss: 1.9325 val_acc: 0.2800 val_f1: 0.2800\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0010 lr: 0.01 train_loss: 1.9418 train_acc: 0.1429 train_f1: 0.1429 time: 0.0785s\n",
      "INFO:root:Epoch: 0010 val_loss: 1.9321 val_acc: 0.3160 val_f1: 0.3160\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0015 lr: 0.01 train_loss: 1.9404 train_acc: 0.1571 train_f1: 0.1571 time: 0.0774s\n",
      "INFO:root:Epoch: 0015 val_loss: 1.9316 val_acc: 0.3180 val_f1: 0.3180\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0020 lr: 0.01 train_loss: 1.9327 train_acc: 0.2500 train_f1: 0.2500 time: 0.0761s\n",
      "INFO:root:Epoch: 0020 val_loss: 1.9308 val_acc: 0.3900 val_f1: 0.3900\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0025 lr: 0.01 train_loss: 1.9354 train_acc: 0.1571 train_f1: 0.1571 time: 0.0765s\n",
      "INFO:root:Epoch: 0025 val_loss: 1.9303 val_acc: 0.3620 val_f1: 0.3620\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0030 lr: 0.01 train_loss: 1.9211 train_acc: 0.3000 train_f1: 0.3000 time: 0.0759s\n",
      "INFO:root:Epoch: 0030 val_loss: 1.9379 val_acc: 0.2940 val_f1: 0.2940\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0035 lr: 0.01 train_loss: 1.9066 train_acc: 0.3429 train_f1: 0.3429 time: 0.0781s\n",
      "INFO:root:Epoch: 0035 val_loss: 1.9347 val_acc: 0.3160 val_f1: 0.3160\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0040 lr: 0.01 train_loss: 1.9013 train_acc: 0.2786 train_f1: 0.2786 time: 0.0771s\n",
      "INFO:root:Epoch: 0040 val_loss: 1.9235 val_acc: 0.3640 val_f1: 0.3640\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0045 lr: 0.01 train_loss: 1.8790 train_acc: 0.3286 train_f1: 0.3286 time: 0.0828s\n",
      "INFO:root:Epoch: 0045 val_loss: 1.9060 val_acc: 0.4140 val_f1: 0.4140\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0050 lr: 0.01 train_loss: 1.8338 train_acc: 0.4714 train_f1: 0.4714 time: 0.0800s\n",
      "INFO:root:Epoch: 0050 val_loss: 1.8745 val_acc: 0.5940 val_f1: 0.5940\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0055 lr: 0.01 train_loss: 1.7947 train_acc: 0.4643 train_f1: 0.4643 time: 0.0838s\n",
      "INFO:root:Epoch: 0055 val_loss: 1.8320 val_acc: 0.6900 val_f1: 0.6900\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0060 lr: 0.01 train_loss: 1.6752 train_acc: 0.5429 train_f1: 0.5429 time: 0.0792s\n",
      "INFO:root:Epoch: 0060 val_loss: 1.7879 val_acc: 0.6300 val_f1: 0.6300\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0065 lr: 0.01 train_loss: 1.6270 train_acc: 0.4786 train_f1: 0.4786 time: 0.0797s\n",
      "INFO:root:Epoch: 0065 val_loss: 1.7243 val_acc: 0.6340 val_f1: 0.6340\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0070 lr: 0.01 train_loss: 1.5454 train_acc: 0.4286 train_f1: 0.4286 time: 0.0760s\n",
      "INFO:root:Epoch: 0070 val_loss: 1.6261 val_acc: 0.7120 val_f1: 0.7120\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0075 lr: 0.01 train_loss: 1.4295 train_acc: 0.4571 train_f1: 0.4571 time: 0.0762s\n",
      "INFO:root:Epoch: 0075 val_loss: 1.5127 val_acc: 0.7360 val_f1: 0.7360\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0080 lr: 0.01 train_loss: 1.2277 train_acc: 0.5714 train_f1: 0.5714 time: 0.0761s\n",
      "INFO:root:Epoch: 0080 val_loss: 1.4126 val_acc: 0.7480 val_f1: 0.7480\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0085 lr: 0.01 train_loss: 1.1056 train_acc: 0.6214 train_f1: 0.6214 time: 0.0785s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch: 0085 val_loss: 1.3170 val_acc: 0.7380 val_f1: 0.7380\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0090 lr: 0.01 train_loss: 1.0884 train_acc: 0.6000 train_f1: 0.6000 time: 0.0794s\n",
      "INFO:root:Epoch: 0090 val_loss: 1.1993 val_acc: 0.7620 val_f1: 0.7620\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0095 lr: 0.01 train_loss: 1.0434 train_acc: 0.6071 train_f1: 0.6071 time: 0.0771s\n",
      "INFO:root:Epoch: 0095 val_loss: 1.1194 val_acc: 0.7680 val_f1: 0.7680\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0100 lr: 0.01 train_loss: 1.0486 train_acc: 0.5500 train_f1: 0.5500 time: 0.0758s\n",
      "INFO:root:Epoch: 0100 val_loss: 1.0589 val_acc: 0.7680 val_f1: 0.7680\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0105 lr: 0.01 train_loss: 0.9647 train_acc: 0.5571 train_f1: 0.5571 time: 0.0761s\n",
      "INFO:root:Epoch: 0105 val_loss: 1.0121 val_acc: 0.7600 val_f1: 0.7600\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0110 lr: 0.01 train_loss: 0.8277 train_acc: 0.6786 train_f1: 0.6786 time: 0.0759s\n",
      "INFO:root:Epoch: 0110 val_loss: 0.9623 val_acc: 0.7660 val_f1: 0.7660\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0115 lr: 0.01 train_loss: 0.9118 train_acc: 0.6000 train_f1: 0.6000 time: 0.0760s\n",
      "INFO:root:Epoch: 0115 val_loss: 0.9117 val_acc: 0.7700 val_f1: 0.7700\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0120 lr: 0.01 train_loss: 0.8403 train_acc: 0.6286 train_f1: 0.6286 time: 0.0762s\n",
      "INFO:root:Epoch: 0120 val_loss: 0.8965 val_acc: 0.7780 val_f1: 0.7780\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0125 lr: 0.01 train_loss: 0.7800 train_acc: 0.6571 train_f1: 0.6571 time: 0.0762s\n",
      "INFO:root:Epoch: 0125 val_loss: 0.8740 val_acc: 0.7880 val_f1: 0.7880\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0130 lr: 0.01 train_loss: 0.8298 train_acc: 0.6000 train_f1: 0.6000 time: 0.0760s\n",
      "INFO:root:Epoch: 0130 val_loss: 0.8445 val_acc: 0.7840 val_f1: 0.7840\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0135 lr: 0.01 train_loss: 0.7723 train_acc: 0.6429 train_f1: 0.6429 time: 0.0763s\n",
      "INFO:root:Epoch: 0135 val_loss: 0.8294 val_acc: 0.7740 val_f1: 0.7740\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0140 lr: 0.01 train_loss: 0.7845 train_acc: 0.6286 train_f1: 0.6286 time: 0.0754s\n",
      "INFO:root:Epoch: 0140 val_loss: 0.8211 val_acc: 0.7660 val_f1: 0.7660\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0145 lr: 0.01 train_loss: 0.7662 train_acc: 0.6000 train_f1: 0.6000 time: 0.0757s\n",
      "INFO:root:Epoch: 0145 val_loss: 0.8192 val_acc: 0.7660 val_f1: 0.7660\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0150 lr: 0.01 train_loss: 0.7771 train_acc: 0.6143 train_f1: 0.6143 time: 0.0775s\n",
      "INFO:root:Epoch: 0150 val_loss: 0.8056 val_acc: 0.7680 val_f1: 0.7680\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0155 lr: 0.01 train_loss: 0.7410 train_acc: 0.6357 train_f1: 0.6357 time: 0.0846s\n",
      "INFO:root:Epoch: 0155 val_loss: 0.8006 val_acc: 0.7820 val_f1: 0.7820\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0160 lr: 0.01 train_loss: 0.7039 train_acc: 0.6786 train_f1: 0.6786 time: 0.0771s\n",
      "INFO:root:Epoch: 0160 val_loss: 0.7945 val_acc: 0.7800 val_f1: 0.7800\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0165 lr: 0.01 train_loss: 0.7472 train_acc: 0.6286 train_f1: 0.6286 time: 0.0841s\n",
      "INFO:root:Epoch: 0165 val_loss: 0.7763 val_acc: 0.7620 val_f1: 0.7620\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0170 lr: 0.01 train_loss: 0.7063 train_acc: 0.7071 train_f1: 0.7071 time: 0.0756s\n",
      "INFO:root:Epoch: 0170 val_loss: 0.7738 val_acc: 0.7700 val_f1: 0.7700\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0175 lr: 0.01 train_loss: 0.7393 train_acc: 0.6286 train_f1: 0.6286 time: 0.0757s\n",
      "INFO:root:Epoch: 0175 val_loss: 0.7709 val_acc: 0.7720 val_f1: 0.7720\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0180 lr: 0.01 train_loss: 0.7000 train_acc: 0.6286 train_f1: 0.6286 time: 0.0757s\n",
      "INFO:root:Epoch: 0180 val_loss: 0.7760 val_acc: 0.7720 val_f1: 0.7720\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0185 lr: 0.01 train_loss: 0.7839 train_acc: 0.5929 train_f1: 0.5929 time: 0.0764s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch: 0185 val_loss: 0.7854 val_acc: 0.7620 val_f1: 0.7620\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0190 lr: 0.01 train_loss: 0.7346 train_acc: 0.5786 train_f1: 0.5786 time: 0.0768s\n",
      "INFO:root:Epoch: 0190 val_loss: 0.7983 val_acc: 0.7620 val_f1: 0.7620\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0195 lr: 0.01 train_loss: 0.7911 train_acc: 0.6429 train_f1: 0.6429 time: 0.0767s\n",
      "INFO:root:Epoch: 0195 val_loss: 0.7940 val_acc: 0.7760 val_f1: 0.7760\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0200 lr: 0.01 train_loss: 0.7981 train_acc: 0.6214 train_f1: 0.6214 time: 0.0758s\n",
      "INFO:root:Epoch: 0200 val_loss: 0.7856 val_acc: 0.7800 val_f1: 0.7800\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0205 lr: 0.01 train_loss: 0.6945 train_acc: 0.6286 train_f1: 0.6286 time: 0.0763s\n",
      "INFO:root:Epoch: 0205 val_loss: 0.7858 val_acc: 0.7720 val_f1: 0.7720\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0210 lr: 0.01 train_loss: 0.6424 train_acc: 0.6643 train_f1: 0.6643 time: 0.0850s\n",
      "INFO:root:Epoch: 0210 val_loss: 0.7897 val_acc: 0.7680 val_f1: 0.7680\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0215 lr: 0.01 train_loss: 0.7426 train_acc: 0.6286 train_f1: 0.6286 time: 0.0759s\n",
      "INFO:root:Epoch: 0215 val_loss: 0.7976 val_acc: 0.7580 val_f1: 0.7580\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0220 lr: 0.01 train_loss: 0.7281 train_acc: 0.6357 train_f1: 0.6357 time: 0.0757s\n",
      "INFO:root:Epoch: 0220 val_loss: 0.7933 val_acc: 0.7600 val_f1: 0.7600\n",
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:Epoch: 0225 lr: 0.01 train_loss: 0.6514 train_acc: 0.6714 train_f1: 0.6714 time: 0.0768s\n",
      "INFO:root:Epoch: 0225 val_loss: 0.7912 val_acc: 0.7680 val_f1: 0.7680\n",
      "INFO:root:Early stopping\n",
      "INFO:root:Optimization Finished!\n",
      "INFO:root:Total time elapsed: 28.0648s\n",
      "INFO:root:Val set results: val_loss: 0.8740 val_acc: 0.7880 val_f1: 0.7880\n",
      "INFO:root:Test set results: test_loss: 0.8494 test_acc: 0.7890 test_f1: 0.7890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.8494, grad_fn=<NllLossBackward0>),\n",
       " 'acc': 0.789,\n",
       " 'f1': 0.7890000000000001}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training and Testing\n",
    "trainer=gz.trainers.Trainer(params,model,optimizer,data)\n",
    "trainer.run()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
